{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db31ed35-156f-4eff-b1ea-6e14cc43b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 11:57:56.738583: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-06 11:57:56.762827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 11:57:57.129609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a57397-1895-4c51-b9bf-eefb449bcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXSEQ = 500\n",
    "NUM_FILTER = 512\n",
    "WINDOW_SIZES = [2,8]\n",
    "DROPOUT = 0.7\n",
    "NUM_HIDDEN = 500\n",
    "BATCH_SIZE  = 256\n",
    "NUM_CLASSES = 2\n",
    "NUM_FEATURE = 1024\n",
    "EPOCHS      = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d63d242-fa90-4377-8835-36024a2e2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepScan(Model):\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t             input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "\t             window_sizes=[4,8,16],\n",
    "\t             num_filters=256,\n",
    "\t             num_hidden=1000):\n",
    "\t\tsuper(DeepScan, self).__init__()\n",
    "\t\t# Add input layer\n",
    "\t\tself.input_layer = tf.keras.Input(input_shape)\n",
    "\t\tself.window_sizes = window_sizes\n",
    "\t\tself.conv2d = []\n",
    "\t\tself.maxpool = []\n",
    "\t\tself.flatten = []\n",
    "\t\tfor window_size in self.window_sizes:\n",
    "\t\t\tself.conv2d.append(\n",
    "                layers.Conv2D(filters=num_filters,\n",
    "    \t\t\t              kernel_size=(1, window_size),\n",
    "\t\t\t                  activation=tf.nn.relu,\n",
    "\t\t\t                  padding='valid',\n",
    "\t\t\t                  bias_initializer=tf.constant_initializer(0.1),\n",
    "\t\t\t                  kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "                             )\n",
    "                             )\n",
    "\t\t\tself.maxpool.append(\n",
    "    \t\t\tlayers.MaxPooling2D(pool_size=(1, MAXSEQ - window_size + 1),\n",
    "\t\t\t                        strides=(1, MAXSEQ),\n",
    "\t\t\t                        padding='valid')\n",
    "                               )\n",
    "\t\t\tself.flatten.append(layers.Flatten())\n",
    "\t\tself.dropout = layers.Dropout(rate=0.7)\n",
    "\t\tself.fc1 = layers.Dense(num_hidden,\n",
    "                        \t\tactivation=tf.nn.relu,\n",
    "\t\t                        bias_initializer=tf.constant_initializer(0.1),\n",
    "\t\t                        kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "                               )\n",
    "\t\tself.fc2 = layers.Dense(NUM_CLASSES,\n",
    "\t\t                        activation='softmax',\n",
    "\t\t                        kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "                               )\n",
    "\n",
    "\t\t# Get output layer with `call` method\n",
    "\t\tself.out = self.call(self.input_layer)\n",
    "\n",
    "\tdef call(self, x, training=False):\n",
    "\t\t_x = []\n",
    "\t\tfor i in range(len(self.window_sizes)):\n",
    "\t\t\tx_conv = self.conv2d[i](x)\n",
    "\t\t\tx_maxp = self.maxpool[i](x_conv)\n",
    "\t\t\tx_flat = self.flatten[i](x_maxp)\n",
    "\t\t\t_x.append(x_flat)\n",
    "\n",
    "\t\tx = tf.concat(_x, 1)\n",
    "\t\tx = self.dropout(x, training=training)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.fc2(x)  #Best Threshold\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1baf1be5-96c9-41be-8ef4-88e8ac916c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(741, 1, 500, 1024)\n",
      "(5908, 1, 500, 1024)\n",
      "(741,)\n",
      "(5908,)\n",
      "(185, 1, 500, 1024)\n",
      "(1478, 1, 500, 1024)\n",
      "(185,)\n",
      "(1478,)\n"
     ]
    }
   ],
   "source": [
    "import import_tests as load_data\n",
    "DATA_TYPE = \"ProtTrans\"\n",
    "MAXSEQ = 500\n",
    "x_train,y_train,x_test,y_test= load_data.MCNN_data_load(DATA_TYPE,MAXSEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d9263e-18ca-4e05-9fc5-e733f654a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1663, 1, 500, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6c3012-7bbe-4434-b0e2-93c3cef9f6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 1, 500, 1024)\n"
     ]
    }
   ],
   "source": [
    "# 假设输入数据形状为 (7000, 1, 500, 1024)\n",
    "# me_train = np.load(\"/mnt/D/jupyter/Evan/vesicular_new/Vesicular_Manuscript_Revision/ESM2_1280_L500/me_train.npy\")\n",
    "\n",
    "# input_data = np.load(\"/mnt/D/jupyter/Evan/vesicular_new/Vesicular_Manuscript_Revision/ESM2_1280_L500/me_train.npy\")\n",
    "\n",
    "# x_test = input_data[\"feature\"]\n",
    "# y_test = input_data[\"label\"]\n",
    "\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "x_test = np.load(\"/mnt/D/jupyter/peter/data/ProtTrans/500/Cytokine_receptor/test.npy\")\n",
    "# x_test = x_test_npz[\"feature\"]\n",
    "print(x_test.shape)\n",
    "\n",
    "\n",
    "# # 创建 DataGenerator，假设 batch_size = 32\n",
    "# batch_size = 32\n",
    "# test_generator = DataGenerator(x_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3804028d-f2ca-4090-9d9e-f8153b260f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 11:58:58.582051: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 361.33MiB (rounded to 378880000)requested by op Mul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-12-06 11:58:58.582077: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2024-12-06 11:58:58.582081: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 72B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582084: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582086: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582089: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 6, Chunks in use: 6. 13.8KiB allocated for chunks. 13.8KiB in use in bin. 11.9KiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582091: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 2, Chunks in use: 2. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 7.8KiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582093: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 8.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582095: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582097: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582099: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582100: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582102: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582105: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 2, Chunks in use: 1. 1.94MiB allocated for chunks. 998.0KiB in use in bin. 998.0KiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582107: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 4, Chunks in use: 3. 7.81MiB allocated for chunks. 5.86MiB in use in bin. 5.86MiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582109: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 0. 2.12MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582111: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 3, Chunks in use: 2. 12.09MiB allocated for chunks. 8.00MiB in use in bin. 8.00MiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582113: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582115: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 4, Chunks in use: 2. 68.00MiB allocated for chunks. 32.00MiB in use in bin. 32.00MiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582127: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582129: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582131: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582133: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 1. 414.43MiB allocated for chunks. 414.43MiB in use in bin. 361.33MiB client-requested in use in bin.\n",
      "2024-12-06 11:58:58.582136: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 361.33MiB was 256.00MiB, Chunk State: \n",
      "2024-12-06 11:58:58.582138: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 531038208\n",
      "2024-12-06 11:58:58.582143: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6000000 of size 256 next 1\n",
      "2024-12-06 11:58:58.582144: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6000100 of size 1280 next 2\n",
      "2024-12-06 11:58:58.582148: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6000600 of size 256 next 3\n",
      "2024-12-06 11:58:58.582149: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6000700 of size 256 next 4\n",
      "2024-12-06 11:58:58.582151: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6000800 of size 256 next 6\n",
      "2024-12-06 11:58:58.582153: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6000900 of size 2048 next 7\n",
      "2024-12-06 11:58:58.582154: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6001100 of size 256 next 5\n",
      "2024-12-06 11:58:58.582156: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6001200 of size 256 next 8\n",
      "2024-12-06 11:58:58.582157: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6001300 of size 2048 next 13\n",
      "2024-12-06 11:58:58.582159: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6001b00 of size 256 next 11\n",
      "2024-12-06 11:58:58.582160: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6001c00 of size 256 next 12\n",
      "2024-12-06 11:58:58.582162: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6001d00 of size 1021952 next 28\n",
      "2024-12-06 11:58:58.582164: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e60fb500 of size 256 next 36\n",
      "2024-12-06 11:58:58.582165: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 78a5e60fb600 of size 8704 next 41\n",
      "2024-12-06 11:58:58.582167: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e60fd800 of size 4096 next 42\n",
      "2024-12-06 11:58:58.582169: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 78a5e60fe800 of size 1012992 next 27\n",
      "2024-12-06 11:58:58.582170: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e61f5d00 of size 2048768 next 20\n",
      "2024-12-06 11:58:58.582172: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e63ea000 of size 2048000 next 21\n",
      "2024-12-06 11:58:58.582174: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65de000 of size 2048 next 17\n",
      "2024-12-06 11:58:58.582175: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65de800 of size 256 next 18\n",
      "2024-12-06 11:58:58.582177: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65de900 of size 256 next 16\n",
      "2024-12-06 11:58:58.582178: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65dea00 of size 256 next 22\n",
      "2024-12-06 11:58:58.582180: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65deb00 of size 256 next 23\n",
      "2024-12-06 11:58:58.582181: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65dec00 of size 256 next 19\n",
      "2024-12-06 11:58:58.582183: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65ded00 of size 256 next 24\n",
      "2024-12-06 11:58:58.582185: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65dee00 of size 2048 next 31\n",
      "2024-12-06 11:58:58.582186: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65df600 of size 2048 next 29\n",
      "2024-12-06 11:58:58.582188: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65dfe00 of size 3840 next 25\n",
      "2024-12-06 11:58:58.582190: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e65e0d00 of size 4096 next 26\n",
      "2024-12-06 11:58:58.582191: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 78a5e65e1d00 of size 2223616 next 9\n",
      "2024-12-06 11:58:58.582193: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6800b00 of size 4194304 next 10\n",
      "2024-12-06 11:58:58.582194: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 78a5e6c00b00 of size 2048000 next 38\n",
      "2024-12-06 11:58:58.582196: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e6df4b00 of size 2048000 next 37\n",
      "2024-12-06 11:58:58.582197: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 78a5e6fe8b00 of size 4292608 next 33\n",
      "2024-12-06 11:58:58.582199: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e7400b00 of size 4194304 next 32\n",
      "2024-12-06 11:58:58.582201: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 78a5e7800b00 of size 20971520 next 15\n",
      "2024-12-06 11:58:58.582202: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5e8c00b00 of size 16777216 next 14\n",
      "2024-12-06 11:58:58.582205: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 78a5e9c00b00 of size 16777216 next 35\n",
      "2024-12-06 11:58:58.582206: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5eac00b00 of size 16777216 next 34\n",
      "2024-12-06 11:58:58.582208: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 78a5ebc00b00 of size 434566400 next 18446744073709551615\n",
      "2024-12-06 11:58:58.582210: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2024-12-06 11:58:58.582212: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 15 Chunks of size 256 totalling 3.8KiB\n",
      "2024-12-06 11:58:58.582215: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-12-06 11:58:58.582218: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 2048 totalling 10.0KiB\n",
      "2024-12-06 11:58:58.582220: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3840 totalling 3.8KiB\n",
      "2024-12-06 11:58:58.582223: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 4096 totalling 8.0KiB\n",
      "2024-12-06 11:58:58.582225: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1021952 totalling 998.0KiB\n",
      "2024-12-06 11:58:58.582228: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 2048000 totalling 3.91MiB\n",
      "2024-12-06 11:58:58.582231: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2048768 totalling 1.95MiB\n",
      "2024-12-06 11:58:58.582234: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 4194304 totalling 8.00MiB\n",
      "2024-12-06 11:58:58.582238: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 16777216 totalling 32.00MiB\n",
      "2024-12-06 11:58:58.582241: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 434566400 totalling 414.43MiB\n",
      "2024-12-06 11:58:58.582245: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 461.29MiB\n",
      "2024-12-06 11:58:58.582248: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 531038208 memory_limit_: 531038208 available bytes: 0 curr_region_allocation_bytes_: 1062076416\n",
      "2024-12-06 11:58:58.582255: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                       531038208\n",
      "InUse:                       483703552\n",
      "MaxInUse:                    483703552\n",
      "NumAllocs:                         117\n",
      "MaxAllocSize:                434566400\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-12-06 11:58:58.582260: W tensorflow/tsl/framework/bfc_allocator.cc:497] *****___****___***************************************************************************xxxxxxxxxx\n",
      "2024-12-06 11:58:58.589359: W tensorflow/core/framework/op_kernel.cc:1818] RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m DeepScan(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, MAXSEQ, NUM_FEATURE), \n\u001b[1;32m      3\u001b[0m                  window_sizes\u001b[38;5;241m=\u001b[39mWINDOW_SIZES, \n\u001b[1;32m      4\u001b[0m                  num_filters\u001b[38;5;241m=\u001b[39mNUM_FILTER, \n\u001b[1;32m      5\u001b[0m                  num_hidden\u001b[38;5;241m=\u001b[39mNUM_HIDDEN)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 构建模型（通过虚拟输入构建）\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m185\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m model(dummy_input)  \u001b[38;5;66;03m# 确保模型结构与训练时一致\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/TF/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/TF/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul]"
     ]
    }
   ],
   "source": [
    "# 初始化模型，并使用与训练时相同的卷积层参数 input_shape=(4, 1, 500, 1280), \n",
    "model = DeepScan(input_shape=(1, MAXSEQ, NUM_FEATURE), \n",
    "                 window_sizes=WINDOW_SIZES, \n",
    "                 num_filters=NUM_FILTER, \n",
    "                 num_hidden=NUM_HIDDEN)\n",
    "\n",
    "# 构建模型（通过虚拟输入构建）\n",
    "dummy_input = tf.random.normal([185, 1, 500, 1024])\n",
    "output = model(dummy_input)  # 确保模型结构与训练时一致\n",
    "print(output.shape)\n",
    "# 加载训练好的权重\n",
    "model.load_weights('/mnt/D/jupyter/peter/my_model_None_filter512_window28_hidden500_weights.h5')\n",
    "print(\"weight loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16d218-d06f-4402-b10c-ba1dde900b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
